{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of neuronets_notes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOYXnKXm1qNG0zXrAdWb0Cf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/perelygin/MFTI/blob/master/neuronets_notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Kbdynz7_4c",
        "colab_type": "text"
      },
      "source": [
        "#Лекция 10. Рекурентные нейронные сети(RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAztqTp58Kjo",
        "colab_type": "text"
      },
      "source": [
        "#Лекция 11. Затухание градиента"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DhAVSfYKKXMo",
        "colab_type": "text"
      },
      "source": [
        "При классификации текста, нам нужно сначала его прочитать и потом выполнить классификацию.  В отличии от задачи генерации текста,  ошибку мы получаем только в конце. В итоге эта ошибка проходит через все итерации(перемножаем производные по всем скрытым состояниям) в обратную сторону. И если одна из производных окажется близкой к 0, то при большом числе итераций, градиент будет стремиться к 0. И это плохо.\n",
        "\n",
        "в RNN матрица W каждый раз одна и та же. И каждый раз ее градиент будет зависеть от того кусочка контекста, который приходил на вход в этой итерации. И градиент по первой итерации будет меньше чем гралиент по последней итерации. Т.е. последняя итреациия будет оказывать больше влияние на наш контекст - происходит забывание информации.\n",
        "\n",
        "Взрывание градиента - это когда мы делаем слишком большой градиентный шаг  и оказываемся в той области, где ошибка еще больше(когда лос-функция не гладкая или есть выбросы в данных).\n",
        "Способы борьбы с этим: образека градиента(gradient clipping) -  когда мы не даем градиенту быть больше чем какое-то число. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH_ARYfxWoGk",
        "colab_type": "text"
      },
      "source": [
        "##RNN и CNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyYJFeg_WwxM",
        "colab_type": "text"
      },
      "source": [
        "биграммы - пары слов. Текст - этоодномерный сигнал, к которому можно применить свертку.\n",
        "Слова в виде векторов(отображение слов в векторное пространство): \n",
        "\n",
        "one-hot(каждому слову - единица в совем разряде) не подходит - большая размерность и много нулевых значений. Кроме того каждое слово отнесено в отдельное измерение и слова становятся независимыми.\n",
        "\n",
        "TF-IDF(term frequency - inverse document frequency ). В языке больше малозначимых слов(предлогов,  междометий и т.д.). Если мы решаем задачу классификации, то слова, которые есть во всех текстах не помогут нам отличить тексты друг от друга. Поэтому мы смотрим как частото встречается слов в конкретном документе и во всех текстах.  Подходит для простых задач.\n",
        "\n",
        "Embeding- операции в пространстве смыслов. Как понять что слово что-то значит? Как понять, что слова похожи между собой или нет? Какую метрику и пространство для этого использовать?\n",
        "Похожие слова- это слова, которые встречаются в одном и том же контексте. Слова определяет свой контекст и слово определяется своим контекстом.По слову предсказываем  его контекст(Игра в ассоциации).\n",
        "На входе One-hot вектор(размерность словаря 100000), потом полносвязанный? слой,  потом soft-max для получения вероятности. После обучения мы получаем пространство - набор весов в скрытом слое. Используя эти веса можно очень быстро получить предсказание. потому что умножается one-hot вектор,  в котором только 1 единица.\n",
        "\n",
        "\n",
        "Как сократить размер словаря:\n",
        "1. Устойчивые словосочетания испольовать как отдельные токены. Например: Российская Федерация\n",
        "2. **subsampling** Так как наша задача предсказать для слова его контекст, то предсказывать контекст, например по предлогам - пустая трата времени.  Поэтому мы оцениваем частоту встречаемости нашего слова во всех тех текстах с которыми мы работали, и на основании этой частоты выкидываем слово с той или иной вероятностью. Вероятность обучить на слове,  в зависимости от его частоты. Если слово встречается часто, то мы редко на нем обучаемся.  Если слово встречается редко, то мы обучаемся на нем часто. Т.е.  чем чаще встречается слово, тем чаще мы его выкидываем.\n",
        "3. **Negativ sampling** тоже самое, но для случая когда у нас очень много классов(столько же сколько и слов). Умножение ошибки на ноль случайным образом. Чаще для тех слов,  которые встречаются чаще"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ra_7c3rGzl5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}